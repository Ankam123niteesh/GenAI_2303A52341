{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4K2aYnbintdkty0jvxuOZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ankam123niteesh/GenAI_2303A52341/blob/main/2303A52341_GenAI_A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1"
      ],
      "metadata": {
        "id": "QC2DrzrdghPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sympy as sp\n",
        "x=sp.symbols('x')\n",
        "f = 5*x**4 + 3*x**2 + 10"
      ],
      "metadata": {
        "id": "HCr35hmlg_JT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(f_str, initial_x, learning_rate, num_iterations):\n",
        "    x = sp.symbols('x')\n",
        "    f_expr = sp.sympify(f_str)\n",
        "    gradient_fn = sp.diff(f_expr, x)\n",
        "    xi = initial_x\n",
        "    for i in range(num_iterations):\n",
        "        gradient = gradient_fn.subs(x, xi)\n",
        "        xi = xi - learning_rate * gradient\n",
        "        if abs(gradient) < 1e-6:\n",
        "            print(f\"Converged at iteration {i+1}\")\n",
        "            break\n",
        "    return xi.evalf(), f_expr.subs(x, xi).evalf()"
      ],
      "metadata": {
        "id": "_V5DeooQg-wV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# f = 5*x**4 + 3*x**2 + 10\n",
        "f_str = input(\"Enter the equation: \")\n",
        "initial_x= int(input(\"Enter the initial value of x: \"))\n",
        "learning_rate= float(input(\"Enter the learning rate: \"))\n",
        "num_iterations= int(input(\"Enter the number of iterations: \"))\n",
        "x,min=gradient_descent(f_str,initial_x, learning_rate, num_iterations)\n",
        "\n",
        "print(f\"The function f(x) = {f_str} achieves its minimum value at x = {x}.\")\n",
        "print(f\"At this point, the minimum value of the function is {min}.\")"
      ],
      "metadata": {
        "id": "UWF3sz-ug-U1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6a3e948-7986-41c5-bc7c-b682c4fa441e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the equation: 5*x**4 + 3*x**2 + 10\n",
            "Enter the initial value of x: 2\n",
            "Enter the learning rate: 0.01\n",
            "Enter the number of iterations: 150\n",
            "The function f(x) = 5*x**4 + 3*x**2 + 10 achieves its minimum value at x = 0.0000244276389904536.\n",
            "At this point, the minimum value of the function is 10.0000000017901.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sympy as sp\n",
        "x=sp.symbols('x')\n",
        "f = 5*x**4 + 3*x**2 + 10\n",
        "derivative = sp.diff(f,x)\n",
        "print(\"Differentiate of Function\")\n",
        "derivative"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "bO1oS2Ctiu0P",
        "outputId": "fcbf322e-46e8-4c0e-9fa8-71cc9b6d45b7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Differentiate of Function\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20*x**3 + 6*x"
            ],
            "text/latex": "$\\displaystyle 20 x^{3} + 6 x$"
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2\n"
      ],
      "metadata": {
        "id": "OAw6ZrvUjuU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_2d(f_str, initial_x, initial_y, learning_rate, num_iterations):\n",
        "\n",
        "    x, y = sp.symbols('x y')\n",
        "    f_expr = sp.sympify(f_str)\n",
        "    gradient_x = sp.diff(f_expr, x)\n",
        "    gradient_y = sp.diff(f_expr, y)\n",
        "    xi, yi = initial_x, initial_y\n",
        "    for i in range(num_iterations):\n",
        "        grad_x = gradient_x.subs({x: xi, y: yi})\n",
        "        grad_y = gradient_y.subs({x: xi, y: yi})\n",
        "        xi = xi - learning_rate * grad_x\n",
        "        yi = yi - learning_rate * grad_y\n",
        "        if abs(grad_x) < 1e-6 and abs(grad_y) < 1e-6:\n",
        "            print(f\"Converged at iteration {i+1}\")\n",
        "            break\n",
        "    return xi.evalf(), yi.evalf(), f_expr.subs({x: xi, y: yi}).evalf()"
      ],
      "metadata": {
        "id": "5PLhRdfYiuw-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_str = \"3*x**2 + 5*exp(-y) + 10\"\n",
        "initial_x = float(input(\"Enter the initial value of x : \"))\n",
        "initial_y = float(input(\"Enter the initial value of y : \"))\n",
        "learning_rate = float(input(\"Enter the learning rate : \"))\n",
        "num_iterations = int(input(\"Enter the number of iterations : \"))\n",
        "\n",
        "x_min, y_min, min_value = gradient_descent_2d(f_str, initial_x, initial_y, learning_rate, num_iterations)\n",
        "\n",
        "print(f\"Minimum value of g(x, y) is {min_value} at x = {x_min}, y = {y_min}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGZ6bKo9iuuM",
        "outputId": "fe183fe3-6a06-4c1e-8009-d794ee279098"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the initial value of x : 1\n",
            "Enter the initial value of y : 1\n",
            "Enter the learning rate : 0.01\n",
            "Enter the number of iterations : 150\n",
            "Minimum value of g(x, y) is 10.4877296623691 at x = 0.0000931489663391464, y = 2.32743196298675.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3"
      ],
      "metadata": {
        "id": "T5LEnByZkiZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_sigmoid(initial_x, learning_rate, num_iterations):\n",
        "    x = sp.symbols('x')\n",
        "    function = 1 / (1 + sp.exp(-x))\n",
        "    gradient_fn = sp.diff(function, x)\n",
        "    xi = initial_x\n",
        "    for i in range(num_iterations):\n",
        "        gradient = gradient_fn.subs(x, xi)\n",
        "        xi = xi - learning_rate * gradient\n",
        "        if abs(gradient) < 1e-6:\n",
        "            print(f\"Converged at iteration {i+1}\")\n",
        "            break\n",
        "    return xi.evalf(), function.subs(x, xi).evalf()"
      ],
      "metadata": {
        "id": "PFnlqZFOiurP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_x = float(input(\"Enter the initial value of x : \"))\n",
        "learning_rate = float(input(\"Enter the learning rate : \"))\n",
        "num_iterations = int(input(\"Enter the number of iterations : \"))\n",
        "\n",
        "x_min, min_value = gradient_descent_sigmoid(initial_x, learning_rate, num_iterations)\n",
        "\n",
        "print(f\"Minimum value of z(x) is {min_value} at x = {x_min}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7VYgBJwkjzi",
        "outputId": "80d868ed-f671-46c1-e53f-bcc6438f493a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the initial value of x : 2\n",
            "Enter the learning rate : 0.01\n",
            "Enter the number of iterations : 150\n",
            "Minimum value of z(x) is 0.862057374914158 at x = 1.83248399021647.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4"
      ],
      "metadata": {
        "id": "yVLihM6rk3O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_linear_model(inputs, expected_outputs, learning_rate, num_iterations):\n",
        "    M, C = 0, 0\n",
        "    n = len(inputs)\n",
        "    for i in range(num_iterations):\n",
        "        gradient_M = sum(-2 * x * (y - (M * x + C)) for x, y in zip(inputs, expected_outputs)) / n\n",
        "        gradient_C = sum(-2 * (y - (M * x + C)) for x, y in zip(inputs, expected_outputs)) / n\n",
        "        M = M - learning_rate * gradient_M\n",
        "        C = C - learning_rate * gradient_C\n",
        "        if abs(gradient_M) < 1e-6 and abs(gradient_C) < 1e-6:\n",
        "            print(f\"Converged at iteration {i+1}\")\n",
        "            break\n",
        "\n",
        "    return M, C"
      ],
      "metadata": {
        "id": "mVhO-FX2kjwP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [1, 2, 3, 4]\n",
        "expected_outputs = [2, 4, 6, 8]\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "\n",
        "M_opt, C_opt = gradient_descent_linear_model(inputs, expected_outputs, learning_rate, num_iterations)\n",
        "\n",
        "print(f\"Optimal values are M = {M_opt}, C = {C_opt}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk_Zd3L-kjtr",
        "outputId": "85c146fb-ab64-4490-8818-4be0b5e2a278"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal values are M = 1.9896587550255742, C = 0.030404521305361965.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o3iAsNECkjq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I36nia_ZkjoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DItvsPi9iufS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}